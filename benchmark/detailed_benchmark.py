import sys
import os
import time
import asyncio
import csv
from datetime import datetime
from collections import defaultdict

# === é…ç½® ===
NUM_RUNS = 5  # æ¯ä¸ªç”¨ä¾‹æµ‹è¯• 5 é
OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))

# === ç¯å¢ƒè·¯å¾„è®¾ç½® ===
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

# === å¯¼å…¥æ•°æ® ===
try:
    from dataset import TEST_EXAMPLES
except ImportError:
    print("âŒ è¯·ç¡®ä¿ benchmark/dataset.py æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

# === å¯¼å…¥æ¶æ„æ„å»ºå™¨ ===
try:
    from src.graph.builder import build_optag_graph as build_legacy_graph
except ImportError:
    build_legacy_graph = None
    print("âš ï¸ æ—§æ¶æ„ (Legacy) æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")

try:
    # æ³¨æ„è¿™é‡Œä½¿ç”¨äº†ç›¸å¯¹å¼•ç”¨çš„ä¿®æ­£ï¼Œç¡®ä¿ graph.py å’Œ nodes.py å·²ç»æ”¹å¥½äº†
    from src.neuro_optagent.graph import build_optagent as build_neuro_graph
except ImportError:
    build_neuro_graph = None
    print("âš ï¸ æ–°æ¶æ„ (Neuro-Symbolic) æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")

# === æ ¸å¿ƒè¿è¡Œå™¨ ===
async def run_profiled_graph(graph, graph_name, case_id, prompt, run_idx):
    """
    è¿è¡Œå›¾å¹¶è®°å½•èŠ‚ç‚¹è€—æ—¶ (å¸¦å®æ—¶æ‰“å°)
    """
    if not graph:
        return []

    print(f"   ğŸš© Run {run_idx}/{NUM_RUNS} Started...")
    
    inputs = {
        "messages": [{"role": "user", "content": prompt}], 
        "problem_statement": prompt,                       
        "correction_count": 0,
        "max_corrections": 3 # Legacy æ¶æ„é‡è¯•æ¬¡æ•°
    }
    
    timeline = []
    start_time = time.time()
    last_checkpoint = start_time
    status = "SUCCESS"
    error_msg = ""
    
    try:
        # stream_mode="updates" è¿”å›æ¯ä¸ªèŠ‚ç‚¹å®Œæˆåçš„çŠ¶æ€å¢é‡
        # subgraphs=True ç¡®ä¿æˆ‘ä»¬èƒ½æ•æ‰åˆ°åµŒå¥—å›¾å†…éƒ¨çš„èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ Legacy å†…éƒ¨çš„èŠ‚ç‚¹ï¼‰
        async for event in graph.astream(inputs, stream_mode="updates", subgraphs=True):
            current_time = time.time()
            
            # event æ ¼å¼é€šå¸¸æ˜¯: (namespace, {node_name: update}) æˆ–è€…ç›´æ¥ {node_name: update}
            # æˆ‘ä»¬é€šè¿‡è§£ææ¥è·å–èŠ‚ç‚¹å
            data = event
            if isinstance(event, tuple):
                # å¤„ç†å­å›¾äº‹ä»¶ (namespace, chunk)
                data = event[1]
            
            if isinstance(data, dict):
                for node_name, state_update in data.items():
                    duration = current_time - last_checkpoint
                    
                    # å®æ—¶æ‰“å°ï¼šå‘Šè¯‰ç”¨æˆ·è·‘åˆ°äº†å“ªé‡Œ
                    print(f"      â±ï¸  Node [{node_name}] finished ({duration:.2f}s)")
                    
                    timeline.append({
                        "case_id": case_id,
                        "run_index": run_idx,
                        "architecture": graph_name,
                        "node": node_name,
                        "duration_seconds": round(duration, 4),
                        "timestamp": datetime.now().strftime("%H:%M:%S"),
                        "status": "SUCCESS",
                        "error_msg": ""
                    })
                    last_checkpoint = current_time
                
    except Exception as e:
        status = "ERROR"
        error_msg = str(e)
        print(f"      âŒ Error encountered: {e}")

    total_time = time.time() - start_time
    print(f"   ğŸ Run {run_idx} Completed in {total_time:.2f}s\n")
    
    # è®°å½•æ€»è€—æ—¶ (E2E)
    timeline.append({
        "case_id": case_id,
        "run_index": run_idx,
        "architecture": graph_name,
        "node": "TOTAL_E2E",
        "duration_seconds": round(total_time, 4),
        "timestamp": datetime.now().strftime("%H:%M:%S"),
        "status": status,
        "error_msg": error_msg
    })
    
    return timeline

# === ä¸»ç¨‹åº ===
async def main():
    print("="*60)
    print(f"ğŸš€ Robustness Benchmark (Sequential Batches: {NUM_RUNS} runs/arch)")
    print("="*60)
    
    print("Building Graphs...")
    legacy_agent = build_legacy_graph() if build_legacy_graph else None
    neuro_agent = build_neuro_graph() if build_neuro_graph else None
    
    all_records = []
    summary_data = defaultdict(lambda: defaultdict(list))
    
    # éå†æµ‹è¯•ç”¨ä¾‹
    for case_id, prompt in TEST_EXAMPLES.items():
        print(f"\n" + "="*40)
        print(f"ğŸ“ TestCase: {case_id}")
        print("="*40)
        
        # === 1. æ‰¹é‡è¿è¡Œ Legacy (Run 1-5) ===
        if legacy_agent:
            print(f"\nğŸ“¦ [Batch Testing] Legacy Architecture")
            print("-" * 30)
            for i in range(1, NUM_RUNS + 1):
                recs = await run_profiled_graph(legacy_agent, "Legacy", case_id, prompt, i)
                all_records.extend(recs)
                # æ”¶é›†æ‘˜è¦æ•°æ®
                total_node = next((r for r in recs if r['node'] == 'TOTAL_E2E'), None)
                if total_node and total_node['status'] == 'SUCCESS':
                    summary_data[case_id]['Legacy'].append(total_node['duration_seconds'])
        
        # === 2. æ‰¹é‡è¿è¡Œ Neuro-Symbolic (Run 1-5) ===
        if neuro_agent:
            print(f"\nğŸ§  [Batch Testing] Neuro-Symbolic Architecture")
            print("-" * 30)
            for i in range(1, NUM_RUNS + 1):
                recs = await run_profiled_graph(neuro_agent, "Neuro-Symbolic", case_id, prompt, i)
                all_records.extend(recs)
                # æ”¶é›†æ‘˜è¦æ•°æ®
                total_node = next((r for r in recs if r['node'] == 'TOTAL_E2E'), None)
                if total_node and total_node['status'] == 'SUCCESS':
                    summary_data[case_id]['Neuro'].append(total_node['duration_seconds'])

    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"benchmark_sequential_{timestamp}.csv"
    csv_path = os.path.join(OUTPUT_DIR, csv_filename)
    
    fieldnames = ["case_id", "run_index", "architecture", "node", "duration_seconds", "timestamp", "status", "error_msg"]
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_records)
            
    print("\n" + "="*60)
    print(f"ğŸ’¾ Raw Data Saved: {csv_path}")
    print("="*60)
    
    # æ‰“å°å¯¹æ¯”æ‘˜è¦
    print("\nğŸ“Š Average Latency Summary (Avg of 5 runs):")
    print(f"{'Case ID':<25} | {'Legacy Avg(s)':<15} | {'Neuro Avg(s)':<15} | {'Diff'}")
    print("-" * 75)
    
    for case_id in TEST_EXAMPLES.keys():
        leg_times = summary_data[case_id].get('Legacy', [])
        neuro_times = summary_data[case_id].get('Neuro', [])
        
        leg_avg = f"{sum(leg_times)/len(leg_times):.2f}" if leg_times else "N/A"
        neuro_avg = f"{sum(neuro_times)/len(neuro_times):.2f}" if neuro_times else "N/A"
        
        diff_str = "-"
        if leg_times and neuro_times:
            diff = (sum(neuro_times)/len(neuro_times)) - (sum(leg_times)/len(leg_times))
            diff_str = f"{diff:+.2f}s"
        
        print(f"{case_id:<25} | {leg_avg:<15} | {neuro_avg:<15} | {diff_str}")

if __name__ == "__main__":
    asyncio.run(main())